
# ğŸ“ˆ RÃ©gression LinÃ©aire â€” Fondamentaux du Machine Learning

La **rÃ©gression linÃ©aire** est un algorithme central en **apprentissage supervisÃ©**, utilisÃ© pour prÃ©dire une **valeur continue** Ã  partir d'une ou plusieurs variables explicatives. Il Ã©tablit une **relation linÃ©aire** entre les donnÃ©es dâ€™entrÃ©e (features) et la variable cible (label).

---

## ğŸ” Qu'est-ce que la rÃ©gression ?

La **rÃ©gression** vise Ã  estimer une **valeur numÃ©rique** plutÃ´t quâ€™Ã  classifier. Exemples dâ€™usage :

- PrÃ©dire le **prix dâ€™une maison** en fonction de sa taille, localisation, etc.
- Estimer la **tempÃ©rature** dâ€™un jour Ã  partir de lâ€™historique mÃ©tÃ©o.
- PrÃ©voir le **trafic web** Ã  partir des dÃ©penses marketing.

---

## ğŸ“ RÃ©gression LinÃ©aire Simple

Relation entre **une seule variable prÃ©dictive** et une **cible continue** :

```python
y = m * x + c
```

- `y` : variable cible prÃ©dite  
- `x` : variable explicative  
- `m` : pente (impact de x sur y)  
- `c` : constante dâ€™interception  

ğŸ¯ Objectif : trouver les meilleurs `m` et `c` pour **minimiser lâ€™erreur** entre les prÃ©dictions et les valeurs rÃ©elles (via la mÃ©thode **des moindres carrÃ©s ordinaires**, OLS).

---

## ğŸ§® RÃ©gression LinÃ©aire Multiple

Lorsque plusieurs variables prÃ©dictives sont utilisÃ©es :

```python
y = b0 + b1*x1 + b2*x2 + ... + bn*xn
```

- `x1, x2, ..., xn` : variables explicatives  
- `b0` : interception  
- `b1, ..., bn` : coefficients associÃ©s  

---

## âš™ï¸ OLS â€“ Ordinary Least Squares

MÃ©thode pour estimer les **coefficients optimaux** :

1. **RÃ©sidus** : diffÃ©rence entre valeur rÃ©elle et valeur prÃ©dite  
2. **Ã‰lÃ©vation au carrÃ©** des rÃ©sidus  
3. **Somme des carrÃ©s des rÃ©sidus (RSS)**  
4. **Minimisation de la RSS** â†’ ligne de "meilleur ajustement"

ğŸ“‰ Lâ€™objectif est de **minimiser lâ€™erreur globale** du modÃ¨le.

---

## ğŸ§  HypothÃ¨ses de la rÃ©gression linÃ©aire

Pour garantir la validitÃ© du modÃ¨le, certaines hypothÃ¨ses doivent Ãªtre respectÃ©es :

| HypothÃ¨se           | Description |
|---------------------|-------------|
| **LinÃ©aritÃ©**        | Relation linÃ©aire entre variables |
| **IndÃ©pendance**     | Les observations sont indÃ©pendantes |
| **HomoscÃ©dasticitÃ©** | Variance constante des erreurs |
| **NormalitÃ©**        | Distribution normale des rÃ©sidus |

âš ï¸ Si ces hypothÃ¨ses sont violÃ©es, les **infÃ©rences statistiques** ou les **prÃ©dictions** peuvent Ãªtre biaisÃ©es.

---

## âœ… Pourquoi utiliser la rÃ©gression linÃ©aire ?

- SimplicitÃ© et **interprÃ©tabilitÃ©**
- Rapide Ã  entraÃ®ner
- Base pour des modÃ¨les plus complexes (ex. : rÃ©gression polynomiale, ridge, lasso)

---

## ğŸ“Œ Ã€ retenir

La rÃ©gression linÃ©aire est une **brique essentielle** en machine learning pour :

- Comprendre la **relation entre variables**
- PrÃ©dire des valeurs continues
- Construire des modÃ¨les robustes pour l'analyse prÃ©dictive

ğŸ‘‰ Ã€ explorer ensuite : **rÃ©gression logistique**, **rÃ©gularisation (L1/L2)**, et **modÃ¨les non linÃ©aires**.
