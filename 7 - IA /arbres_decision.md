
# ğŸŒ³ Arbres de DÃ©cision â€” Classification et RÃ©gression en Machine Learning

Les **arbres de dÃ©cision** sont des algorithmes dâ€™apprentissage supervisÃ© utilisÃ©s pour la **classification** et la **rÃ©gression**. Leur structure en arbre les rend **intuitifs, explicables** et faciles Ã  interprÃ©ter.

---

## ğŸ§  Principe

Un arbre de dÃ©cision apprend Ã  **prendre des dÃ©cisions** en posant une sÃ©rie de **questions simples** basÃ©es sur les attributs des donnÃ©es.

Exemple : jouer au tennis ?
- Est-ce ensoleillÃ© ?
- Est-ce venteux ?
- Est-ce humide ?
â†’ RÃ©ponse : oui / non

---

## ğŸŒ² Structure dâ€™un Arbre

- **NÅ“ud racine** : point de dÃ©part, contient tout lâ€™ensemble de donnÃ©es
- **NÅ“uds internes** : critÃ¨res de dÃ©cision (features)
- **Feuilles** : dÃ©cisions finales ou prÃ©dictions

---

## ğŸ› ï¸ Construction de lâ€™Arbre

Ã€ chaque nÅ“ud, lâ€™arbre choisit le **meilleur attribut** pour diviser les donnÃ©es. Cette dÃ©cision est basÃ©e sur :

- **Gini impurity**
- **Entropie**
- **Gain dâ€™information**

ğŸ¯ Objectif : crÃ©er des sous-ensembles les plus **purs** possibles.

---

## ğŸ“‰ Gini Impurity

```python
Gini(S) = 1 - Î£ (p_i)^2
```

- Plus Gini est faible, plus le sous-ensemble est pur.
- Exemple : 30 A, 20 B â†’ Gini = 0.48

---

## ğŸŒ€ Entropie

```python
Entropy(S) = - Î£ p_i * log2(p_i)
```

- Mesure lâ€™**incertitude**
- Exemple : mÃªme dataset â†’ Entropy â‰ˆ 0.97

---

## ğŸ“Š Gain dâ€™Information

```python
InfoGain(S, A) = Entropy(S) - Î£ (|S_v| / |S|) * Entropy(S_v)
```

- Mesure la **rÃ©duction dâ€™entropie**
- Le meilleur split = plus grand gain d'information

---

## ğŸŒ¿ Croissance de l'Arbre

Lâ€™arbre grandit rÃ©cursivement tant que :

- ğŸ **Profondeur maximale** non atteinte
- ğŸ“‰ **Nombre minimum** de donnÃ©es par nÅ“ud respectÃ©
- âœ… **PuretÃ©** dâ€™un nÅ“ud atteinte (tous les points dâ€™une classe)

---

## ğŸ¸ Exemple : Jouer au tennis

Features :
- Outlook (Sunny, Overcast, Rainy)
- TempÃ©rature (Hot, Mild, Cool)
- HumiditÃ© (High, Normal)
- Vent (Weak, Strong)

Cible : `Play Tennis` (Yes / No)

ğŸ” Lâ€™algorithme :
- Calcule le **gain dâ€™information** pour chaque attribut
- CrÃ©e des **branches** pour chaque valeur possible
- RÃ©pÃ¨te pour chaque sous-ensemble jusquâ€™au critÃ¨re dâ€™arrÃªt

---

## ğŸ“ HypothÃ¨ses sur les donnÃ©es

Les arbres de dÃ©cision ont trÃ¨s **peu dâ€™hypothÃ¨ses** :

| HypothÃ¨se                       | DÃ©tail |
|--------------------------------|--------|
| **Pas de linÃ©aritÃ© requise**   | GÃ¨re relations complexes |
| **Pas de normalitÃ©**           | Aucune distribution spÃ©cifique requise |
| **TolÃ©rance aux outliers**     | Moins sensibles aux valeurs extrÃªmes |

---

## âœ… Ã€ retenir

- ModÃ¨le **interprÃ©table**, visuel, explicable
- Convient Ã  des donnÃ©es **catÃ©gorielles et continues**
- Sensible Ã  lâ€™**overfitting** (Ã  rÃ©guler par `max_depth`, `min_samples_leaf`, etc.)

ğŸ‘‰ Utilisations : analyse de risque, crÃ©dit, diagnostic, jeux de donnÃ©es avec structure logique.
